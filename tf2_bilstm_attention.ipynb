{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow 2.0 下 bilstm + attention 实现文本分类 demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import datetime\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import traceback\n",
    "import time \n",
    "import json\n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers,Input\n",
    "from tensorflow.keras.layers import Dense,LSTM,Bidirectional,Dropout,Embedding,BatchNormalization\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = pd.read_csv('E:/Pycharm/calss_comment/data/train.csv')\n",
    "test = pd.read_csv('E:/Pycharm/calss_comment/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('E:/Pycharm/text_summary/data/train.csv')\n",
    "test_df = pd.read_csv('E:/Pycharm/text_summary/data/test.csv')\n",
    "\n",
    "x_train = train_df['article'].values\n",
    "y_train = train_df['summarization'].values\n",
    "x_test = test_df['article'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[\"text\"]\n",
    "y_train = train['label']\n",
    "x_test = test[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abbreviations(text):\n",
    "    texts = []\n",
    "    for item in text:\n",
    "        item = item.lower().replace(\"it's\", \"it is\").replace(\"i'm\", \"i am\").replace(\"he's\", \"he is\").replace(\"she's\",\n",
    "                                                                                                             \"she is\") \\\n",
    "            .replace(\"we're\", \"we are\").replace(\"they're\", \"they are\").replace(\"you're\", \"you are\").replace(\"that's\",\n",
    "                                                                                                            \"that is\") \\\n",
    "            .replace(\"this's\", \"this is\").replace(\"can't\", \"can not\").replace(\"don't\", \"do not\").replace(\"doesn't\",\n",
    "                                                                                                         \"does not\") \\\n",
    "            .replace(\"we've\", \"we have\").replace(\"i've\", \" i have\").replace(\"isn't\", \"is not\").replace(\"won't\",\n",
    "                                                                                                       \"will not\") \\\n",
    "            .replace(\"hasn't\", \"has not\").replace(\"wasn't\", \"was not\").replace(\"weren't\", \"were not\").replace(\"let's\",\n",
    "                                                                                                              \"let us\") \\\n",
    "            .replace(\"didn't\", \"did not\").replace(\"hadn't\", \"had not\").replace(\"waht's\", \"what is\").replace(\"couldn't\",\n",
    "                                                                                                            \"could not\") \\\n",
    "            .replace(\"you'll\", \"you will\").replace(\"you've\", \"you have\")\n",
    "\n",
    "        item = item.replace(\"'s\", \"\")\n",
    "        texts.append(item)\n",
    "\n",
    "    return texts\n",
    "\n",
    "#删除标点符号及其它字符\n",
    "\n",
    "\n",
    "def clear_review(text):\n",
    "    texts = []\n",
    "    for item in text:\n",
    "        item = item.replace(\"<br /><br />\", \"\")\n",
    "        item = re.sub(\"[^a-zA-Z]\", \" \", item.lower())\n",
    "        texts.append(\" \".join(item.split()))\n",
    "    return texts\n",
    "\n",
    "#删除停用词　＋　词形还原\n",
    "\n",
    "\n",
    "def stemed_words(text):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemma = WordNetLemmatizer()\n",
    "    texts = []\n",
    "    for item in text:\n",
    "        words = [lemma.lemmatize(w, pos='v') for w in item.split() if w not in stop_words]\n",
    "        texts.append(\" \".join(words))\n",
    "    return texts\n",
    "\n",
    "#文本预处理\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = replace_abbreviations(text)\n",
    "    text = clear_review(text)\n",
    "    text = stemed_words(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_texts = preprocess(x_train)\n",
    "test_texts = preprocess(x_test)\n",
    "\n",
    "max_features = 6000\n",
    "texts = train_texts + test_texts\n",
    "#分词\n",
    "tok = Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(texts)\n",
    "#序列\n",
    "list_tok = tok.texts_to_sequences(texts)\n",
    "\n",
    "maxlen = 1024\n",
    "\n",
    "seq_tok = pad_sequences(list_tok, maxlen=maxlen)\n",
    "\n",
    "x_train = seq_tok[:len(train_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(raw_y, num_classes):\n",
    "    index = np.array(raw_y)\n",
    "    class_cnt = num_classes #np.max(index) + 1 \n",
    "    out = np.zeros((index.shape[0], class_cnt))\n",
    "    out[np.arange(index.shape[0]), index] = 1\n",
    "    return out \n",
    "def load_sample(fn, max_seq_len, word_dict, num_classes):\n",
    "    text_df = pd.read_csv(fn)\n",
    "    raw_y = []\n",
    "    raw_x = []\n",
    "    for i in range(len(text_df)):\n",
    "        label = text_df['label'][i]\n",
    "        raw_y.append(int(label))\n",
    "\n",
    "        text = text_df['text'][i]\n",
    "        text_len = len(text)\n",
    "        x = np.zeros(max_seq_len, dtype = np.int32)\n",
    "    if text_len <= max_seq_len:\n",
    "          for i in range(text_len):\n",
    "            x[i] = word_dict[text[i]]\n",
    "    else:\n",
    "          for i in range(text_len - max_seq_len, text_len):\n",
    "            x[i - text_len + max_seq_len] = word_dict[text[i]]\n",
    "    raw_x.append(x)\n",
    "\n",
    "    all_x = np.array(raw_x)\n",
    "    all_y = one_hot_encode(raw_y, num_classes)\n",
    "    return all_x, all_y \n",
    "def batch_iter(x, y, batch_size = 16):\n",
    "    data_len = len(x)\n",
    "    num_batch = (data_len + batch_size - 1) // batch_size\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuff = x[indices]\n",
    "    y_shuff = y[indices]\n",
    "    for i in range(num_batch):\n",
    "        start_offset = i*batch_size \n",
    "        end_offset = min(start_offset + batch_size, data_len)\n",
    "        yield i, num_batch, x_shuff[start_offset:end_offset], y_shuff[start_offset:end_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnAttentionLayer(layers.Layer):\n",
    "  def __init__(self, attention_size, drop_rate):\n",
    "    super().__init__()\n",
    "    self.attention_size = attention_size\n",
    "    self.dropout = Dropout(drop_rate, name = \"rnn_attention_dropout\")\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.attention_w = self.add_weight(name = \"atten_w\", shape = (input_shape[-1], self.attention_size), initializer = tf.random_uniform_initializer(), dtype = \"float32\", trainable = True)\n",
    "    self.attention_u = self.add_weight(name = \"atten_u\", shape = (self.attention_size,), initializer = tf.random_uniform_initializer(), dtype = \"float32\", trainable = True)\n",
    "    self.attention_b = self.add_weight(name = \"atten_b\", shape = (self.attention_size,), initializer = tf.constant_initializer(0.1), dtype = \"float32\", trainable = True)    \n",
    "    super().build(input_shape)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    x = tf.tanh(tf.add(tf.tensordot(inputs, self.attention_w, axes = 1), self.attention_b))\n",
    "    x = tf.tensordot(x, self.attention_u, axes = 1)\n",
    "    x = tf.nn.softmax(x)\n",
    "    weight_out = tf.multiply(tf.expand_dims(x, -1), inputs)\n",
    "    final_out = tf.reduce_sum(weight_out, axis = 1) \n",
    "    drop_out = self.dropout(final_out, training = training)\n",
    "    return drop_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(layers.Layer):\n",
    "  def __init__(self, rnn_size, drop_rate):\n",
    "    super().__init__()\n",
    "    fwd_lstm = LSTM(rnn_size, return_sequences = True, go_backwards= False, dropout = drop_rate, name = \"fwd_lstm\")\n",
    "    bwd_lstm = LSTM(rnn_size, return_sequences = True, go_backwards = True, dropout = drop_rate, name = \"bwd_lstm\")\n",
    "    self.bilstm = Bidirectional(merge_mode = \"concat\", layer = fwd_lstm, backward_layer = bwd_lstm, name = \"bilstm\")\n",
    "    #self.bilstm = Bidirectional(LSTM(rnn_size, activation= \"relu\", return_sequences = True, dropout = drop_rate))\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    outputs = self.bilstm(inputs, training = training)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, num_classes, drop_rate, vocab_size, embedding_size, rnn_size, attention_size):\n",
    "    super().__init__()\n",
    "    self.embedding_layer = Embedding(vocab_size, embedding_size, embeddings_initializer = \"uniform\", name = \"embeding_0\")\n",
    "    self.rnn_layer = RnnLayer(rnn_size, drop_rate)\n",
    "    self.attention_layer = RnnAttentionLayer(attention_size, drop_rate)\n",
    "    self.rnn_layer = RnnLayer(rnn_size//2, drop_rate)\n",
    "    self.attention_layer = RnnAttentionLayer(attention_size, drop_rate)\n",
    "    self.dense_layer = Dense(num_classes, activation = \"softmax\", kernel_regularizer=keras.regularizers.l2(0.001), name = \"dense_1\")\n",
    "\n",
    "  def call(self, input_x, training):\n",
    "    x = self.embedding_layer(input_x)\n",
    "    x = self.rnn_layer(x, training = training)\n",
    "    x = self.attention_layer(x, training = training)\n",
    "    x = self.dense_layer(x)\n",
    "    return x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop(patience=0, min_delta=0, monitor='val_loss'):\n",
    "    '''\n",
    "    使用early stop的方法，当loss不再下降时，停止训练\n",
    "    :param patience: 当loss不再下降时继续训练的batch数量\n",
    "    :param min_delta: loss的阈值，loss需要下降到该值以下\n",
    "    :param monitor: 需要监视的指标，默认为loss\n",
    "    :return:\n",
    "    '''\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=monitor,\n",
    "            min_delta=min_delta,\n",
    "            patience=patience,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "398/398 [==============================] - 1550s 4s/step - loss: 1.1056 - acc: 0.6594 - val_loss: 0.6285 - val_acc: 0.8341\n",
      "Epoch 2/5\n",
      "398/398 [==============================] - 1026s 3s/step - loss: 0.5002 - acc: 0.8629 - val_loss: 0.5332 - val_acc: 0.8579\n",
      "Epoch 3/5\n",
      "398/398 [==============================] - 1003s 3s/step - loss: 0.3694 - acc: 0.9026 - val_loss: 0.4978 - val_acc: 0.8669\n",
      "Epoch 4/5\n",
      "398/398 [==============================] - 962s 2s/step - loss: 0.2807 - acc: 0.9301 - val_loss: 0.4984 - val_acc: 0.8713\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17f61149208>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(11, drop_rate = 0.05, vocab_size = 6000, \n",
    "              embedding_size = 256, rnn_size = 128, attention_size = 128)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "callbacks = early_stop()\n",
    "model.fit(x_train, y_train, batch_size=32,epochs=5,validation_split=0.3,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(x):\n",
    "        \"\"\"Generate class predictions for the input samples.\n",
    "\n",
    "        The input samples are processed batch by batch.\n",
    "\n",
    "        # Arguments\n",
    "            x: input data, as a Numpy array or list of Numpy arrays\n",
    "                (if the model has multiple inputs).\n",
    "            batch_size: integer.\n",
    "            verbose: verbosity mode, 0 or 1.\n",
    "\n",
    "        # Returns\n",
    "            A numpy array of class predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        if x.shape[-1] > 1:\n",
    "            return x.argmax(axis=-1)\n",
    "        else:\n",
    "            return (x > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = seq_tok[len(train_texts):]\n",
    "y_pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_classes(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['result'] = y_pred\n",
    "result.to_csv('lstm_att1.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
